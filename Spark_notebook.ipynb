{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5139cf05",
   "metadata": {},
   "source": [
    "# Spark notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511dd04c",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Content\n",
    "- [1 - Introduction](#1)\n",
    "- [2 - Spark Features and components](#2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c02f6",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 Introduction\n",
    "what is Apache Spark?\n",
    "Apache Spark is an open-source data processing engine to store and process data in real time across various clusters of computers using simple programming constructs.\n",
    "\n",
    "--Support various languages(R, Python, Jave, Scala)\n",
    "\n",
    "### Hadoop     vs       Spark\n",
    "-- Hadoop processes data using MapReduce. Hadoop is quite slow because it is a batch oriented operation and it is time consuming.\n",
    "However, spark can process the same data 100 times faster than MapReduce as it is a in-memory computing framework. \n",
    "\n",
    "-- Hadoop performs batch processing of data, Spark perfoms both batch processing and real-time or near real-time(real-time means the process of data as it comes in or streaming kind of data) processing of data.\n",
    "\n",
    "-- Hadoop has more lines of code, it is written in Java and takes more time to execute it. Spark has fewer lines of code as it is implemented in Scala.\n",
    "\n",
    "-- Hadoop support Kerberos which make it hard to manager it. Spark supports authentication via a shared secret. It can also run on YARN leveraging the capatibility of Kerberos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652fe6f",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 Spark Features and components\n",
    "\n",
    "### Spark Features\n",
    "\n",
    "-- **Fast processing**, Spark contains ResilientDistributed Datasets(RDD)(Resilient, it is exsiting for a short period of time, distributed across nodes, load and process datasets) which save time taken in reading, and writing operations and hence, it runs almost ten times to hundred times faster than Hadoop.\n",
    "\n",
    "-- **In-memory computing**, (difference between caching and in-memory, caching mainly support read ahead mechanism where you have your data pre-loaded so that it can benefit further queries, however, when we are saying about in-memory computing, we are talking about lazy evaluation, data being loaded into memory only and only when a specific kind of action is involved.) where data is stored in RAM, RAM is not only used for processing but also used for storage. so it can acess data quickly and accelerate the speed of analytics. \n",
    "\n",
    "-- **Flexiable**. It supports multiple languages and allows the developers to wirte applications in Jave, Scala, Python, R.\n",
    "\n",
    "-- **Fault tolerance**. Spark contains Resilient distributed Datasets(RDD, or you could say execution logic or you could say temporary datasets which initially do not have any data loaded and data will be loaded to rdd's only when execution is happening.), that are designed to handle the failure of any worker node in the cluster. it ensures that the loss of data reduces to zero.\n",
    "\n",
    "-- **Better analytics** Spark has a rich set of SQL queries, machine learning algorithms, complex analytics, etc. With all these functionalities, analytics can be performed better.\n",
    "\n",
    "### Components of Apache Spark\n",
    "\n",
    "-- **Spark core**: is embedded with RDDs, the base engine for large-scale parallel and distributed data processing, it is responsible for memory mangagement, fault recovery, scheduling distributing and monitoring jobs on a cluster, interacting with storage systems. \n",
    "\n",
    "----Note that spark itself does not have its own storage, it relies on storage that storage could be your hdfs that is Hadoop ditributed file system, it could be a database like no SQL database such HBase or it could be any other database say our DBMS from where you could connect your spark and fetch, extract data.\n",
    "\n",
    "-- **Spark SQL** framework component is used for structed and semi-structed data processing.\n",
    "\n",
    "-- **Spark streaming** allows your spark streaming applications which will not only works on data which is being streamed in or data which is constantly getting generated, but you would also transform, analyze or process the data as it comes in small chunks. \n",
    "\n",
    "-- **Spark MLlib** a set of libraries which allows data scientists to build their machine learning algorithms so that they can do predicative analysis or prescriptive, preemptive analytics. or build recommendations systems. \n",
    "\n",
    "--**Spark GraphX** allows you to do graph based processing. for some data naturally has a network kind of flow so data could be represented in the form of graphs, network related data, that is data can be networked together which can have some relationship, Like Facebook or Linkedin. where you have one person connected to antoher person or one company connected to another company. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c8e39",
   "metadata": {},
   "source": [
    "#### Resilient Distributed Dataset(RDD)\n",
    "RDD -> Transformation(these are operations such as map, filter, join, unions, ...etc.) that are performed on an RDD that yields a new RDD containing the result.\n",
    "> val x = sc.textfile,    create an empty RDD at node\n",
    "\n",
    "> val y = x.map(),        create an empty child RDD \n",
    "\n",
    "> val z = y.filter()      create one more RDD,one more step in tha last tag,  a series of steps will be executed. No execuation is happending right now, only and only when you invoke action.\n",
    "\n",
    "RDD -> Action(these are operations such as reduce, first, count) that return a value after running a computation on an RDD\n",
    "> z.count(), trigger the action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e402e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the spark packages\n",
    "import pyspark as spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db2dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7747035",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark' has no attribute 'createDataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2240/57331746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Every record contains a label and feature vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"features\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Split the data into train/test datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m.80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyspark' has no attribute 'createDataFrame'"
     ]
    }
   ],
   "source": [
    "# Every record contains a label and feature vector\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "# Split the data into train/test datasets\n",
    "train_df, test_df = df.randomSplit([.80, .20], seed=42)\n",
    "\n",
    "# Set hyperparameters for the algorithm\n",
    "rf = RandomForestRegressor(numTrees=100)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model = rf.fit(train_df)\n",
    "\n",
    "# Generate predictions on the test dataset.\n",
    "model.transform(test_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968f4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
